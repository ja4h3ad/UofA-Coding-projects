{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a25678c9993c1c2bbf0167f2ff03c982",
     "grade": false,
     "grade_id": "header-instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Tips\n",
    "- To avoid unpleasant surprises, I suggest you _run all cells in their order of appearance_ (__Cell__ $\\rightarrow$ __Run All__).\n",
    "\n",
    "\n",
    "- If the changes you've made to your solution don't seem to be showing up, try running __Kernel__ $\\rightarrow$ __Restart & Run All__ from the menu.\n",
    "\n",
    "\n",
    "- Before submitting your assignment, make sure everything runs as expected. First, restart the kernel (from the menu, select __Kernel__ $\\rightarrow$ __Restart__) and then **run all cells** (from the menu, select __Cell__ $\\rightarrow$ __Run All__).\n",
    "\n",
    "## Reminder\n",
    "\n",
    "- Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name, UA email, and collaborators below:\n",
    "\n",
    "\n",
    "\n",
    "Several of the cells in this notebook are **read only** to ensure instructions aren't unintentionally altered.  \n",
    "\n",
    "If you can't edit the cell, it is probably intentional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Tim Dentry\"\n",
    "# University of Arizona email address\n",
    "EMAIL = \"tdentry@email.arizona.edu\"\n",
    "# Names of any collaborators.  Write N/A if none.\n",
    "COLLABORATORS = \"Andrew Quadro; we talked theory and ideas for approaches.  Jackson Mostoller; we traded ideas about how to construct feature dictionaries given the criteria\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0783621da2f047c6360f2ec0d56f121c",
     "grade": false,
     "grade_id": "cell-e35b85c2416e40f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Scratchpad\n",
    "\n",
    "You are welcome to create new cells (see the __Cell__ menu) to experiment and debug your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c80ac423030cfa372644d7cd456061af",
     "grade": false,
     "grade_id": "cell-955f8133afe96b26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e07f9ac61f4be6a57b6961cde23a6d58",
     "grade": false,
     "grade_id": "cell-a2292c2fbc4cf52e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Mini Python tutorial\n",
    "\n",
    "This course uses Python 3.8.\n",
    "\n",
    "Below is a very basic (and incomplete) overview of the Python language... \n",
    "\n",
    "For those completely new to Python, [this section of the official documentation may be useful](https://docs.python.org/3.8/library/stdtypes.html#common-sequence-operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfcd9f827d4855d02514b2e54ba32077",
     "grade": false,
     "grade_id": "cell-d6593132353238c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "e\n",
      "l\n",
      "l\n",
      "o\n",
      "[2, 3, 4, 5]\n",
      "[2]\n",
      "hello, Josuke!\n",
      "Howdy, partner!\n",
      "13\n",
      "Hi, Fred!\n",
      "[('radical', 4), ('analysis', 7), ('bighorn', 12), ('bounce', 32)]\n",
      "[('analysis', 7), ('bighorn', 12), ('bounce', 32), ('radical', 4)]\n"
     ]
    }
   ],
   "source": [
    "# This is a comment.  \n",
    "# Any line starting with # will be interpreted as a comment\n",
    "\n",
    "# this is a string assigned to a variable\n",
    "greeting = \"hello\"\n",
    "\n",
    "# If enclosed in triple quotes, strings can also be multiline:\n",
    "\n",
    "\"\"\"\n",
    "I'm a multiline\n",
    "string.\n",
    "\"\"\"\n",
    "\n",
    "# let's use a for loop to print it letter by letter\n",
    "for letter in greeting:\n",
    "    print(letter)\n",
    "    \n",
    "# Did you notice the indentation there?  Whitespace matters in Python!\n",
    "\n",
    "# here's a list of integers\n",
    "\n",
    "numbers = [1, 2, 3, 4]\n",
    "\n",
    "# let's add one to each number using a list comprehension\n",
    "# and assign the result to a variable called res\n",
    "# list comprehensions are used widely in Python (they're very Pythonic!)\n",
    "\n",
    "res = [num + 1 for num in numbers]\n",
    "\n",
    "# let's confirm that it worked\n",
    "print(res)\n",
    "\n",
    "# now let's try spicing things up using a conditional to filter out all values greater than or equal to 3...\n",
    "print([num for num in res if not num >= 3])\n",
    "\n",
    "# Python 3.7 introduced \"f-strings\" as a convenient way of formatting strings using templates\n",
    "# For example ...\n",
    "name = \"Josuke\"\n",
    "\n",
    "print(f\"{greeting}, {name}!\")\n",
    "\n",
    "# f-strings are f-ing convenient!\n",
    "\n",
    "\n",
    "# let's look at defining functions in Python..\n",
    "\n",
    "def greet(name):\n",
    "    print(f\"Howdy, {name}!\")\n",
    "\n",
    "# here's how we call it...\n",
    "\n",
    "greet(\"partner\")\n",
    "\n",
    "# let's add a description of the function...\n",
    "\n",
    "def greet(name):\n",
    "    \"\"\"\n",
    "    Prints a greeting given some name.\n",
    "    \n",
    "    :param name: the name to be addressed in the greeting\n",
    "    :type name: str\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"Howdy, {name}!\")\n",
    "    \n",
    "# I encourage you to use docstrings!\n",
    "\n",
    "# Python introduced support for optional type hints in v3.5.\n",
    "# You can read more aobut this feature here: https://docs.python.org/3.8/library/typing.html\n",
    "# let's give it a try...\n",
    "def add_six(num: int) -> int:\n",
    "    return num + 6\n",
    "\n",
    "# this should print 13\n",
    "print(add_six(7))\n",
    "\n",
    "# Python also has \"anonymous functions\" (also known as \"lambda\" functions)\n",
    "# take a look at the following code:\n",
    "\n",
    "greet_alt = lambda name: print(f\"Hi, {name}!\")\n",
    "\n",
    "greet_alt(\"Fred\")\n",
    "\n",
    "# lambda functions are often passed to other functions\n",
    "# For example, they can be used to specify how a sequence should be sorted\n",
    "# let's sort a list of pairs by their second element\n",
    "pairs = [(\"bounce\", 32), (\"bighorn\", 12), (\"radical\", 4), (\"analysis\", 7)]\n",
    "# -1 is last thing in some sequence, -2 is the second to last thing in some seq, etc.\n",
    "print(sorted(pairs, key=lambda pair: pair[-1]))\n",
    "\n",
    "# we can sort it by the first element instead\n",
    "# NOTE: python indexing is zero-based\n",
    "print(sorted(pairs, key=lambda pair: pair[0]))\n",
    "\n",
    "# You can learn more about other core data types and their methods here: \n",
    "# https://docs.python.org/3.8/library/stdtypes.html\n",
    "\n",
    "# Because of its extensive standard library, Python is often described as coming with \"batteries included\".  \n",
    "# Take a look at these \"batteries\": https://docs.python.org/3.8/library/\n",
    "\n",
    "# You now know enough to complete this homework assignment (or at least where to look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2dbde2a5d52dfc3a7056fcac987d9613",
     "grade": false,
     "grade_id": "base-imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Iterator, Sequence, Text, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import spmatrix, vstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import itertools\n",
    "import pytest\n",
    "\n",
    "# an NDArray is either a numpy array (ndarray) or a scipy sparse matrix (spmatrix)\n",
    "NDArray  = Union[np.ndarray, spmatrix]\n",
    "# type aliases for sequences of strings\n",
    "# we'll use this type alias for our tokens\n",
    "TokenSeq = Sequence[Text]\n",
    "# ...and this one for our POS tags\n",
    "TagSeq   = Sequence[Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77dc67bfff712b9f453e67434966f28c",
     "grade": false,
     "grade_id": "random-seed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14a290184893defd5bc434e9dced5b05",
     "grade": false,
     "grade_id": "answer-imports",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Add your imports here (ex. classes from scikit-learn)\n",
    "# YOUR CODE HERE\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from typing import Dict\n",
    "from itertools import chain\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "814440fb9d374286ac65fb88a9fbaaa5",
     "grade": false,
     "grade_id": "md-read-data",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `read_ptbtagged`\n",
    "\n",
    "First, we'll implement a function to read in our data.  Be careful!  Your first implementation might be off by 1..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "199f2e63eda7347a3a77412a9e805e15",
     "grade": false,
     "grade_id": "code-read-data",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_ptbtagged(ptbtagged_path: str) -> Iterator[Tuple[TokenSeq, TagSeq]]:\n",
    "    \"\"\"\n",
    "    Reads sentences from a Penn TreeBank .tagged file.\n",
    "    Each sentence is a sequence of tokens and part-of-speech tags.\n",
    "\n",
    "    Penn TreeBank .tagged files contain one token per line, with an empty line\n",
    "    marking the end of each sentence. Each line is composed of a token, a tab\n",
    "    character, and a part-of-speech tag. Here is an example:\n",
    "\n",
    "        What\tWP\n",
    "        's\tVBZ\n",
    "        next\tJJ\n",
    "        ?\t.\n",
    "\n",
    "        Slides\tNNS\n",
    "        to\tTO\n",
    "        illustrate\tVB\n",
    "        Shostakovich\tNNP\n",
    "        quartets\tNNS\n",
    "        ?\t.\n",
    "\n",
    "    :param ptbtagged_path: The path of a Penn TreeBank .tagged file, formatted\n",
    "    as above.\n",
    "    :return: An iterator over sentences, where each sentence is a tuple of\n",
    "    a sequence of tokens and a corresponding sequence of part-of-speech tags.\n",
    "    \"\"\"\n",
    "    \n",
    "    finalSequence=[]# Iterator\n",
    "    \n",
    "    with open(ptbtagged_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        lineCount=len(lines) # counts the number of lines in the entire file being read\n",
    "        tokenSequence = []\n",
    "        posSequence = []\n",
    "        for i, line in enumerate(lines): #counting the lines and parts\n",
    "            parts = line.split('\\t') #split on tab\n",
    "            if(parts[0] in [\"\\n\",\"\"]):\n",
    "                finalSequence.append ((tokenSequence, posSequence))\n",
    "                tokenSequence = []\n",
    "                posSequence = []\n",
    "                # the above meets the condition of looping to the last line\n",
    "                \n",
    "            else:\n",
    "                tokenSequence.append(parts[0]) # [What, 's, ...]\n",
    "                posSequence.append(parts[1][:-1]) #[WP, VBZ,...]\n",
    "                if (i == lineCount-1):  \n",
    "                    finalSequence.append ((tokenSequence, posSequence))\n",
    "                    # meets the condition of the last line being hit\n",
    "          \n",
    "    return finalSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify part-of-speech tags from the Penn Treebank\n",
    "PTB_TAGS = {\n",
    "    \"#\", \"$\", \"''\", \"``\", \",\", \"-LRB-\", \"-RRB-\", \".\", \":\", \"CC\", \"CD\", \"DT\",\n",
    "    \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NN\", \"NNP\", \"NNPS\",\n",
    "    \"NNS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"SYM\", \"TO\",\n",
    "    \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = DictVectorizer(sparse=False)\n",
    "# D = [{'token': 'Pierre', 'pos-1': '<s>'}, {'token': 'Vinken', 'pos-1': 'NNP'}, {'token': ',', 'pos-1': 'NNP'}, {'token': '61', 'pos-1': ','}, {'token': 'years', 'pos-1': 'CD'}, {'token': 'old', 'pos-1': 'NNS'}, {'token': ',', 'pos-1': 'JJ'}, {'token': 'will', 'pos-1': ','}, {'token': 'join', 'pos-1': 'MD'}, {'token': 'the', 'pos-1': 'VB'}, {'token': 'board', 'pos-1': 'DT'}, {'token': 'as', 'pos-1': 'NN'}, {'token': 'a', 'pos-1': 'IN'}, {'token': 'nonexecutive', 'pos-1': 'DT'}, {'token': 'director', 'pos-1': 'JJ'}, {'token': 'Nov.', 'pos-1': 'NN'}, {'token': '29', 'pos-1': 'NNP'}, {'token': '.', 'pos-1': 'CD'}]\n",
    "# X = v.fit_transform(D)\n",
    "# X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "efa8d1241e08a01c3dd278c43ea150b3",
     "grade": false,
     "grade_id": "cell-b2bb58db7fd31606",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## `Classifier`\n",
    "\n",
    "Now we'll implement our classifier using the `Classifier` class.  Underlyingly, your classifier will make use of `sklearn` (see the attributes defined in the `__init__` method in the skeleton below).  \n",
    "\n",
    "**HINT**: When encoding your features, be careful to only fit once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3480c824beb94e213b9f2e07e8bf0eb",
     "grade": false,
     "grade_id": "code-classifier",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Our MEMM\n",
    "class Classifier:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the classifier.\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "        # Minimally, you must include the following features:\n",
    "        # `token` (the current word) \n",
    "        # `pos-1` (the prior tag) \n",
    "        self.feature_encoder = DictVectorizer()\n",
    "        self._feature_index = {}\n",
    "        # multinomial logistic regression\n",
    "        self.model = LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\")\n",
    " \n",
    "\n",
    "\n",
    "    def identify_features (self, tagged_sentences: Iterator[Tuple[TokenSeq, TagSeq]]):\n",
    "        features = []\n",
    "        for sen in tagged_sentences:  \n",
    "            tokens = sen[0]\n",
    "            pos = sen[1]\n",
    "            tok_lis_len = len(tokens)\n",
    "            \n",
    "            for i in range(tok_lis_len):\n",
    "                token = tokens[i]\n",
    "                start = \"<s>\"\n",
    "                if i > 0:\n",
    "                    start = pos[i-1]\n",
    "                features.append({\"token\":  token, \"pos-1\":  start})\n",
    "        return features\n",
    "\n",
    "    def train(self, tagged_sentences: Iterator[Tuple[TokenSeq, TagSeq]]) -> Tuple[NDArray, NDArray]:\n",
    "        \"\"\"\n",
    "        Trains the classifier on the part-of-speech tagged sentences,\n",
    "        and returns the feature matrix and label vector on which it was trained.\n",
    "\n",
    "        The feature matrix should have one row per training token. The number\n",
    "        of columns is up to the implementation, but there must at least be 1\n",
    "        feature for each token, named \"token=T\", where \"T\" is the token string,\n",
    "        and one feature for the part-of-speech tag of the preceding token,\n",
    "        named \"pos-1=P\", where \"P\" is the part-of-speech tag string, or \"<s>\" if\n",
    "        the token was the first in the sentence. For example, if the input is:\n",
    "\n",
    "            What\tWP\n",
    "            's\tVBZ\n",
    "            next\tJJ\n",
    "            ?\t.\n",
    "\n",
    "        Then the first row in the feature matrix should have features for\n",
    "        \"token=What\" and \"pos-1=<s>\", the second row in the feature matrix\n",
    "        should have features for \"token='s\" and \"pos-1=WP\", etc. The alignment\n",
    "        between these feature names and the integer columns of the feature\n",
    "        matrix is given by the `feature_index` method below.\n",
    "\n",
    "        The label vector should have one entry per training token, and each\n",
    "        entry should be an integer. The alignment between part-of-speech tag\n",
    "        strings and the integers in the label vector is given by the\n",
    "        `label_index` method below.\n",
    "\n",
    "        :param tagged_sentences: An iterator over sentences, where each sentence\n",
    "        is a tuple of a sequence of tokens and a corresponding sequence of\n",
    "        part-of-speech tags.\n",
    "        \n",
    "        :return: A tuple of (feature-matrix, label-vector).\n",
    "        \"\"\"\n",
    "        X = list(tagged_sentences)\n",
    "        \n",
    "        pos_list = [pos for _, pos in X]\n",
    "        label_vector = self.label_encoder.fit_transform(list(chain(*pos_list)))\n",
    "    \n",
    "        \n",
    "        \n",
    "        label_vector = self.label_encoder.fit_transform(list(chain(*pos_list)))\n",
    "        feature_matrix = self.feature_encoder.fit_transform(self.identify_features(X))\n",
    "        \n",
    "        self.model.fit(feature_matrix, label_vector)\n",
    "        \n",
    "        self._feature_index = {feature_name: idx \n",
    "                             for idx, feature_name in \n",
    "                             enumerate(self.feature_encoder.feature_names_)}\n",
    "        return feature_matrix, label_vector\n",
    "\n",
    "        \n",
    "                                                        \n",
    "    def feature_index(self, feature: Text) -> int:\n",
    "        \"\"\"\n",
    "        Returns the column index corresponding to the given named feature.\n",
    "\n",
    "        The `train` method should always be called before this method is called.\n",
    "\n",
    "        :param feature: The string name of a feature.\n",
    "        \n",
    "        :return: The column index of the feature in the feature matrix returned\n",
    "        by the `train` method.\n",
    "        \"\"\"\n",
    "        # may need to come back to this one\n",
    "#         self.feature_index = {feat_name:  item for item, feat_name \n",
    "#                              in enumerate(self.feature_encoder.feature_names_)}\n",
    "        return self._feature_index[feature]\n",
    "                                        \n",
    "        \n",
    "    def label_index(self, label: Text) -> int:\n",
    "        \"\"\"\n",
    "        Returns the integer corresponding to the given part-of-speech tag\n",
    "\n",
    "        The `train` method should always be called before this method is called.\n",
    "\n",
    "        :param label: The part-of-speech tag string.\n",
    "        \n",
    "        :return: The integer for the part-of-speech tag, to be used in the label\n",
    "        vector returned by the `train` method.\n",
    "        \"\"\"\n",
    "        if label ==\"<s>\":\n",
    "            return len(self.label_encoder.classes_)\n",
    "        return self.label_encoder.transform([label])[0]\n",
    "\n",
    "    def predict(self, tokens: TokenSeq) -> TagSeq:\n",
    "        \"\"\"\n",
    "        Predicts part-of-speech tags for the sequence of tokens.\n",
    "\n",
    "        This method delegates to either `predict_greedy` or `predict_viterbi`.\n",
    "        The implementer may decide which one to delegate to.\n",
    "\n",
    "        :param tokens: A sequence of tokens representing a sentence.\n",
    "        \n",
    "        :return: A sequence of part-of-speech tags, one for each token.\n",
    "        \"\"\"\n",
    "        _, pos_tags = self.predict_greedy(tokens)\n",
    "        # _, _, pos_tags = self.predict_viterbi(tokens)\n",
    "        return pos_tags\n",
    "    \n",
    "    def more_features(self, tokens):\n",
    "        f_matrix = []\n",
    "        pos = []\n",
    "        start_pos = \"<s>\"\n",
    "        pos_t_2 = \"<s>\"\n",
    "        tok_1 = \"<starting_state>\"\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            if i > 0:\n",
    "                tok_1 = tokens[i - 1]\n",
    "                \n",
    "            vect = self.feature_encoder.transform([{\"token\": token, \"token-1\": tok_1,\n",
    "                \"pos-1\": start_pos,\"pos-2\": pos_t_2}])\n",
    "            \n",
    "            if i > 1:\n",
    "                pos_t_2 = start_pos\n",
    "            \n",
    "            tag_prediction = self.model.predict(vect)\n",
    "            start_pos = self.label_encoder.inverse_transform(tag_prediction)[0]\n",
    "            \n",
    "            f_matrix.append(vect.todense())\n",
    "            pos.append(start_pos)\n",
    "\n",
    "        return np.vstack(f_matrix), pos\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    def predict_greedy(self, tokens: TokenSeq) -> Tuple[NDArray, TagSeq]:\n",
    "        \"\"\"\n",
    "        Predicts part-of-speech tags for the sequence of tokens using a\n",
    "        greedy algorithm, and returns the feature matrix and predicted tags.\n",
    "\n",
    "        Each part-of-speech tag is predicted one at a time, and each prediction\n",
    "        is considered a hard decision, that is, when predicting the\n",
    "        part-of-speech tag for token i, the model will assume that its\n",
    "        prediction for token i-1 is correct and unchangeable.\n",
    "\n",
    "        The feature matrix should have one row per input token, and be formatted\n",
    "        in the same way as the feature matrix in `train`.\n",
    "\n",
    "        :param tokens: A sequence of tokens representing a sentence.\n",
    "        \n",
    "        :return: The feature matrix and the sequence of predicted part-of-speech\n",
    "        tags (one for each input token).\n",
    "        \"\"\"\n",
    "        return self.more_features(tokens)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb0b0eda0c3f2dd2ba77c812ca6b55c5",
     "grade": false,
     "grade_id": "ptb-tags",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# part-of-speech tags from the Penn Treebank\n",
    "PTB_TAGS = {\n",
    "    \"#\", \"$\", \"''\", \"``\", \",\", \"-LRB-\", \"-RRB-\", \".\", \":\", \"CC\", \"CD\", \"DT\",\n",
    "    \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NN\", \"NNP\", \"NNPS\",\n",
    "    \"NNS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"SYM\", \"TO\",\n",
    "    \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['a', 'b', 'c'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict = {\"a\": 1, \"b\": 2, \"c\":3}\n",
    "test_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d72a64679957dba3c3774e94eb42c81",
     "grade": false,
     "grade_id": "md-test-read-data",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test `.read_ptbtagged()` (3 pts)\n",
    "\n",
    "Tests that you read in a) the correct number of sentences and tokens from the training data, b) that all `PTB_TAGS` were found in that partition of the data, and c) each token has exactly one corresponding tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c33bd1559491c2b90679a0743621c2b4",
     "grade": true,
     "grade_id": "test-read-data",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_read_ptbtagged():\n",
    "    # keep a counter here (instead of enumerate) in case the iterator is empty\n",
    "    token_count = 0\n",
    "    sentence_count = 0\n",
    "    for sentence in read_ptbtagged(\"data/PTBSmall/train.tagged\"):\n",
    "        assert len(sentence) == 2\n",
    "        tokens, pos_tags = sentence\n",
    "        assert len(tokens) == len(pos_tags)\n",
    "        assert all(pos in PTB_TAGS for pos in pos_tags)\n",
    "        token_count += len(tokens)\n",
    "        sentence_count += 1\n",
    "    assert token_count == 191969\n",
    "    assert sentence_count == 8020\n",
    "\n",
    "    # check the sentence count in the dev set too\n",
    "    found = sum(1 for _ in read_ptbtagged(\"data/PTBSmall/dev.tagged\")) \n",
    "    expected = 3106\n",
    "    assert found == expected, f\"Expected {expected} sentences, but found {found}\"\n",
    "    \n",
    "test_read_ptbtagged()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a84f6da13a4ac1c2fc880000518390de",
     "grade": true,
     "grade_id": "no-peeking",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def no_peeking() -> bool:\n",
    "    train_data = set(tuple(s) for s, tags in read_ptbtagged(\"data/PTBSmall/train.tagged\"))\n",
    "    dev_data   = set(tuple(s) for s, tags in read_ptbtagged(\"data/PTBSmall/dev.tagged\"))\n",
    "    if len(train_data) < 10:\n",
    "        print(\"Something is wrong with the training data\")\n",
    "        return False\n",
    "    if len(dev_data) < 10:\n",
    "        print(\"Something is wrong with the dev data\")\n",
    "        return False\n",
    "    \n",
    "    for dev_ex in dev_data:\n",
    "        if dev_ex in train_data:\n",
    "            print(dev_ex)\n",
    "            print(\"Dev data should not overlap with train data!\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# ensure the train and dev data is well-formed\n",
    "assert no_peeking() is True, \"Problem with train and/or dev data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2fcfce5931f1fa947cfdb7a437446850",
     "grade": false,
     "grade_id": "md-test-features",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test features (5 pts)\n",
    "\n",
    "This test ensures you are, per the definition of MEMM, minimally representing **token** and **prior tag** ($t_{i-1}$) features.  \n",
    "\n",
    "Use the special symbol `<s>` to represent the prior tag of the first token in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73bb9b1a88027c59213aa14af31a2276",
     "grade": true,
     "grade_id": "test-features",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_feature_vectors():\n",
    "    clf       = Classifier()\n",
    "    ptb_train = read_ptbtagged(\"data/PTBSmall/train.tagged\")\n",
    "    ptb_train = itertools.islice(ptb_train, 2)  # just the first 2 sentences\n",
    "    features_matrix, labels_vector = clf.train(ptb_train)\n",
    "    # num. tokens\n",
    "    assert features_matrix.shape[0] == 31\n",
    "    assert labels_vector.shape[0] == 31\n",
    "\n",
    "    # train.tagged starts with\n",
    "    # Pierre\tNNP\n",
    "    # Vinken\tNNP\n",
    "    # ,\t,\n",
    "    # 61\tCD\n",
    "    # years\tNNS\n",
    "    # old\tJJ\n",
    "    assert features_matrix[4, clf.feature_index(\"token=years\")] == 1\n",
    "    assert features_matrix[4, clf.feature_index(\"token=old\")] == 0\n",
    "    assert features_matrix[4, clf.feature_index(\"pos-1=CD\")] == 1\n",
    "    assert features_matrix[4, clf.feature_index(\"pos-1=NNS\")] == 0\n",
    "    assert features_matrix[0, clf.feature_index(\"pos-1=<s>\")] == 1\n",
    "    assert labels_vector[3] == clf.label_index(\"CD\")\n",
    "    assert labels_vector[4] == clf.label_index(\"NNS\")\n",
    "test_feature_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "354c6985b109e27dd20b3dbecfaff9a0",
     "grade": false,
     "grade_id": "md-test-greedy-decoding",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Test greedy decoding (5pts)\n",
    "\n",
    "In the greedy decoding approach, each tag is predicted one at a time, and each prediction is considered a **hard** decision.  In other words, when predicting the tag for token $t_{i}$, the model will assume that its prediction for the prior token $t_{i-1}$ is correct and unchangeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fd83a94de32643a135021a5b341edec",
     "grade": true,
     "grade_id": "test-greedy-decoding",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_predict_greedy():\n",
    "    clf        = Classifier()\n",
    "    ptb_train  = read_ptbtagged(\"data/PTBSmall/train.tagged\")\n",
    "    ptb_train  = itertools.islice(ptb_train, 2)  # just the 1st 2 sentences\n",
    "    clf.train(ptb_train)\n",
    "\n",
    "    tokens = \"Vinken is a director .\".split()\n",
    "    features_matrix, pos_tags = clf.predict_greedy(tokens)\n",
    "\n",
    "    # check that there is one feature vector per POS tag\n",
    "    assert features_matrix.shape[0] == len(pos_tags)\n",
    "\n",
    "    # check that all POS tags are in the PTB tagset\n",
    "    assert all(pos_tag in PTB_TAGS for pos_tag in pos_tags)\n",
    "\n",
    "    def last_pos_index(ptb_tag):\n",
    "        return clf.feature_index(\"pos-1=\" + ptb_tag)\n",
    "\n",
    "    # check that the first word (\"The\") has no pos-1 feature\n",
    "    for ptb_tag in {\"NNP\", \",\", \"CD\", \"NNS\", \"JJ\", \"MD\", \"VB\", \"DT\", \"NN\", \"IN\",\n",
    "                    \"VBZ\", \"VBG\"}:\n",
    "        assert features_matrix[0, last_pos_index(ptb_tag)] == 0\n",
    "\n",
    "    # check that the remaining words have the correct pos-1 features\n",
    "    for i, pos_tag in enumerate(pos_tags[:-1]):\n",
    "        assert features_matrix[i + 1, last_pos_index(pos_tag)] > 0\n",
    "\n",
    "test_predict_greedy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75a647a1b64d3441f7d11cb7c540ec37",
     "grade": false,
     "grade_id": "md-min-min-accuracy",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Minimum accuracy (4pts)\n",
    "\n",
    "Your model should achieve >= 92% acccuracy against the first 100 sentences of the Penn Treebank development partition.  To achieve this accuracy, you may need to include additional contextual features (i.e., features that represent information about the surrounding words and/or tags).\n",
    "\n",
    "**WARNING**: _this test may be slow to run (2 min.+)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "facaafa1042963b46467084b94f6f94e",
     "grade": true,
     "grade_id": "test-min-accuracy",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "89.0% accuracy on first 100 sentences of PTB dev\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pl/k80fpf9s4f9_3rp8hnpw5x0m0000gq/T/ipykernel_13924/3905444770.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mmin_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.92\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmin_accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_accuracy():\n",
    "    clf       = Classifier()\n",
    "    ptb_train = read_ptbtagged(\"data/PTBSmall/train.tagged\")\n",
    "    clf.train(ptb_train)\n",
    "\n",
    "    total_count   = 0\n",
    "    correct_count = 0\n",
    "    ptb_dev = read_ptbtagged(\"data/PTBSmall/dev.tagged\")\n",
    "    first_n = 100\n",
    "    ptb_dev = itertools.islice(ptb_dev, first_n)  # just the 1st n sentences\n",
    "    for tokens, pos_tags in ptb_dev:\n",
    "        total_count += len(tokens)\n",
    "        predicted_tags = clf.predict(tokens)\n",
    "        assert len(predicted_tags) == len(pos_tags)\n",
    "        for predicted_tag, true_tag in zip(predicted_tags, pos_tags):\n",
    "            if predicted_tag == true_tag:\n",
    "                correct_count += 1\n",
    "    accuracy = correct_count / total_count\n",
    "\n",
    "    # print out performance\n",
    "    sg = f\"\\n{accuracy:.1%} accuracy on first {first_n} sentences of PTB dev\"\n",
    "    print(sg)\n",
    "    return accuracy\n",
    "\n",
    "# ensure solution meets min. accuracy\n",
    "min_accuracy = 0.92\n",
    "accuracy = test_accuracy()\n",
    "assert accuracy >= min_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
